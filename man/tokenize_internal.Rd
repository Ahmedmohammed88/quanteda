% Generated by roxygen2: do not edit by hand
% Please edit documentation in R/tokenize.R
\name{tokenize_internal}
\alias{tokenize_internal}
\alias{tokenize}
\alias{tokenize_default}
\alias{tokenize_word}
\alias{tokenize_character}
\alias{tokenize_sentence}
\alias{tokenize_fasterword}
\alias{tokenize_fastestword}
\title{quanteda tokenizers}
\usage{
tokenize_default(x)

tokenize_word(x)

tokenize_character(x)

tokenize_sentence(x)

tokenize_fasterword(x)

tokenize_fastestword(x)
}
\arguments{
\item{x}{(named) character; input texts}
}
\value{
a list of characters corresponding to the (most conservative) tokens,
including whitespace where applicable.
}
\description{
Internal methods for tokenization providing default and legacy methods for
text segmentation.
}
\examples{
txt <- c(doc1 = "Tweet https://quanteda.io using @quantedainit and #rstats.",
         doc2 = "The £1,000,000 question.",
         doc3 = "毎日 #quanteda を使用してください！",
         doc4 = "Line 1.\nLine2\n\nLine3.")
tokenize_default(txt)
tokenize_fasterword(txt)
tokenize_fastest_word(txt)
tokenize_sentence(txt)
tokenize_character(txt[2])
}
\keyword{internal}
\keyword{tokens}
