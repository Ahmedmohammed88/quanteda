% Generated by roxygen2: do not edit by hand
% Please edit documentation in R/tokens.R
\name{tokens}
\alias{tokens}
\title{Construct a tokens object}
\usage{
tokens(
  x,
  what = "default",
  remove_punct = FALSE,
  remove_symbols = FALSE,
  remove_numbers = FALSE,
  remove_url = FALSE,
  remove_separators = FALSE,
  split_tags = FALSE,
  split_hyphens = FALSE,
  split_currency = FALSE,
  include_docvars = TRUE,
  verbose = quanteda_options("verbose"),
  ...
)
}
\arguments{
\item{x}{a (uniquely) named list of characters or a \link{tokens} object; or a
\link{corpus}, or a character object that will be tokenized}

\item{what}{character; which tokenizer to use.  The "default" is the version
2 \pkg{quanteda} tokenizer.  Legacy tokenizers (version < 2) are also supported.
See the Details and quanteda Tokenizer below.}

\item{remove_punct}{logical; if \code{TRUE} remove all characters in the Unicode
"Punctuation" \verb{[P]} class, with exceptions for those used as prefixes for
valid social media tags if \code{preserve_tags = TRUE}}

\item{remove_symbols}{logical; if \code{TRUE} remove all characters in the Unicode
"Symbol" \verb{[S]} class}

\item{remove_numbers}{logical; if \code{TRUE} remove tokens that consist only of
numbers, but not words that start with digits, e.g. \verb{2day}}

\item{remove_url}{logical; if \code{TRUE} find and eliminate URLs beginning with
http(s) -- see section "Dealing with URLs".}

\item{remove_separators}{logical; if \code{TRUE} remove separators and separator
characters (Unicode "Separator" \verb{[Z]} and "Control" \verb{[C]} categories)}

\item{split_tags}{logical; keep (social media) tags intact, such as "#hashtags" and
"#usernames", even when punctuation will be removed.  The rules defining a
valid "tag" can be found
\href{https://www.hashtags.org/featured/what-characters-can-a-hashtag-include/}{here}
for hashtags and
\href{https://help.twitter.com/en/managing-your-account/twitter-username-rules}{here}
for usernames.}

\item{split_hyphens}{logical; if \code{TRUE}, split words that are connected by
hyphenation and hyphenation-like characters in between words, e.g.
\code{"self-aware"} becomes \code{c("self", "-", "aware")}}

\item{split_currency}{logical; if \code{TRUE}, split currency symbols from the numbers
with which they are associated, e.g. "£15,000" becomes \code{c("£", "15,000")}}

\item{include_docvars}{if \code{TRUE}, pass docvars through to the tokens object.
Only applies when tokenizing \link{corpus} objects.}

\item{verbose}{if \code{TRUE}, print timing messages to the console; off by
default}

\item{...}{additional arguments not used}
}
\value{
\pkg{quanteda} \code{tokens} class object, by default a serialized list of
integers corresponding to a vector of types.
}
\description{
Construct a tokens object, either by importing a named list of characters
from an external tokenizer, or by calling the internal \pkg{quanteda}
tokenizer.
}
\details{
\code{tokens()} works on tokens class objects, which means that the removal rules
can be applied post-tokenization, although it should be noted that it will
not be possible to remove things that are not present.  For instance, if the
\code{tokens} object has already had punctuation removed, then \code{tokens(x, remove_punct = TRUE)} will have no additional effect.
}
\section{Details}{

As of version 2, the choice of tokenizer is left more to the user,
and \code{tokens()} is treated more as a constructor (from a named list) than a
tokenizer. This allows users to use any other tokenizer that returns a
named list, and to use this as an input to \code{tokens()}, with removal and
splitting rules applied after this has been constructed (passed as
arguments).  These removal and splitting rules are conservative and will
not remove or split anything, however, unless the user requests it.

Using external tokenizers is best done by piping the output from these
other tokenizers into the \code{tokens()} constructor, with additional removal
and splitting options applied at the construction stage.  These will only
have an effect,
however, if the tokens exist for which removal is specified at in the
\code{tokens()} call.  For instance, it is impossible to remove punctuation if
the input list to \code{tokens()} already had its punctuation tokens removed at
the external tokenization stage.

To construct a tokens object from a list with no additional processing, call
\code{\link[=as.tokens]{as.tokens()}} instead of \code{tokens()}.

Recommended tokenizers are those from the \pkg{tokenizers} package, which
are generally faster than the default (built-in) tokenizer but always
splits infix hyphens, or \pkg{spacyr}.
}

\section{quanteda tokenizer}{

There is also a default tokenizer, if the input to \code{tokens()} is a
character or \link{corpus} input, based on
\link[stringi:stri_split_boundaries]{stri_split_boundaries}, but that improves
on this by: not splitting words with infix hyphens (e.g. "self-aware"),
not splitting currency symbols from their numbers, not splitting social
media tag characters (#hashtags and @usernames), and preserving URLs and
email addressed

For backward compatibility, the following older tokenizers are also supported
through \code{what}:
\describe{ \item{\code{"word"}}{(recommended default) version < 2 word tokenizer.}
\item{\code{"fasterword"}}{splits on whitespace and control characters, using
\code{stringi::stri_split_charclass(x, "[\\\\p{Z}\\\\p{C}]+")}}
\item{\code{"fastestword"}}{splits on the space character, using
\code{stringi::stri_split_fixed(x, " ")}}
\item{\code{"character"}}{tokenization into individual characters}
\item{\code{"sentence"}}{sentence segmenter based on \pkg{stringi}} }
}

\examples{
txt <- c(doc1 = "A sentence, showing how tokens() works.",
         doc2 = "@quantedainit and #textanalysis on Twitter.",
         doc3 = "Self-documenting code??",
         doc4 = "£1,000,000 for 50¢ is gr8 4ever \U0001f600")
tokens(txt)

# removing punctuation marks and tags
tokens(txt[1:2], remove_punct = TRUE)
tokens(txt[1:2], remove_punct = TRUE, split_tags = FALSE)

# splitting hyphenated words
tokens(txt[3])
tokens(txt[3], split_hyphens = TRUE)

# symbols and numbers
tokens(txt[4])
tokens(txt[4], remove_numbers = TRUE)
tokens(txt[4], remove_symbols = TRUE)

# using other tokenizers
tokens(tokenizers::tokenize_words(txt[4]), remove_symbols = TRUE)
tokenizers::tokenize_words(txt, lowercase = FALSE, strip_punct = FALSE) \%>\%
    tokens(remove_symbols = TRUE)
tokenizers::tokenize_characters(txt[3], strip_non_alphanum = FALSE) \%>\%
    tokens(remove_punct = TRUE)
tokenizers::tokenize_sentences(
    "The quick brown fox.  It jumped over the lazy dog.") \%>\%
    tokens()

}
\seealso{
\code{\link[=tokens_ngrams]{tokens_ngrams()}}, \code{\link[=tokens_skipgrams]{tokens_skipgrams()}}, \code{\link[=as.list.tokens]{as.list.tokens()}}, \code{\link[=as.tokens]{as.tokens()}}
}
\keyword{tokens}
