% Generated by roxygen2: do not edit by hand
% Please edit documentation in R/tokens.R
\name{tokens}
\alias{tokens}
\title{Construct a tokens object}
\usage{
tokens(
  x,
  remove_punct = FALSE,
  remove_symbols = FALSE,
  remove_numbers = FALSE,
  remove_url = FALSE,
  preserve_tags = !remove_punct,
  split_infix_hyphens = FALSE,
  include_docvars = TRUE,
  verbose = quanteda_options("verbose"),
  ...
)
}
\arguments{
\item{x}{a (uniquely) named list of characters or a \link{tokens} object; or a
\link{corpus}, or a character object that will be tokenized}

\item{remove_punct}{logical; if \code{TRUE} remove all characters in the Unicode
"Punctuation" \verb{[P]} class, with exceptions for those used as prefixes for
valid social media tags if \code{preserve_tags = TRUE}}

\item{remove_symbols}{logical; if \code{TRUE} remove all characters in the Unicode
"Symbol" \verb{[S]} class}

\item{remove_numbers}{logical; if \code{TRUE} remove tokens that consist only of
numbers, but not words that start with digits, e.g. \verb{2day}}

\item{remove_url}{logical; if \code{TRUE} find and eliminate URLs beginning with
http(s) -- see section "Dealing with URLs".}

\item{preserve_tags}{keep (social media) tags intact, such as "#hashtags" and
"#usernames" even when punctuation will be removed.  The rules defining a
valid "tag" can be found
\href{https://www.hashtags.org/featured/what-characters-can-a-hashtag-include/}{here}
for hashtags and
\href{https://help.twitter.com/en/managing-your-account/twitter-username-rules}{here}
for usernames.}

\item{split_infix_hyphens}{logical; if \code{TRUE} split words that are connected by
hyphenation and hyphenation-like characters in between words, e.g.
\code{"self-aware"} becomes \code{c("self", "-", "aware")}.  Default is \code{FALSE} to
preserve such words as is, with the hyphens.}

\item{include_docvars}{if \code{TRUE}, pass docvars through to the tokens object.
Only applies when tokenizing \link{corpus} objects.}

\item{verbose}{if \code{TRUE}, print timing messages to the console; off by
default}

\item{...}{additional arguments not used}
}
\value{
\pkg{quanteda} \code{tokens} class object, by default a serialized list of
integers corresponding to a vector of types.
}
\description{
Construct a tokens object, either by importing a named list of characters
from an external tokenizer, or by calling the internal \pkg{quanteda}
tokenizer.
}
\details{
\code{tokens()} works on tokens class objects, which means that the removal rules
can be applied post-tokenization, although it should be noted that it will
not be possible to remove things that are not present.  For instance, if the
\code{tokens} object has already had punctuation removed, then \code{tokens(x, remove_punct = TRUE)} will have no additional effect.

While \pkg{quanteda} has a its own tokenizer, users may prefer to
use other tokenizers that return a named list as input.  This can be done
by piping the output from these other tokenizers into the \code{tokens()}
constructor, with additional removal options applied at the construction
stage.  This will only have an effect, however, if the tokens exist for
which removal is specified at in the \code{tokens()} call.  For instance, it is
impossible to remove punctuation if the input list to \code{tokens()} already
had its punctuation tokens removed at the external tokenization stage.

Recommended tokenizers are those from the \pkg{tokenizers} package, which
are generally faster than the default (built-in) tokenizer but always
splits infix hyphens, or \pkg{spacyr}.
}
\examples{
txt <- c(doc1 = "A sentence, showing how tokens() works.",
         doc2 = "@quantedainit and #textanalysis on Twitter.",
         doc3 = "Self-documenting code??",
         doc4 = "£1,000,000 for 50¢ is gr8 4ever \U0001f600")
tokens(txt)
tmp <- tokens(txt)

# removing punctuation marks and tags
tokens(txt[1:2], remove_punct = TRUE)
tokens(txt[1:2], remove_punct = TRUE, preserve_tags = TRUE)

# split hyphenated words
tokens(txt[3], split_infix_hyphens = TRUE)

# symbols and numbers
tokens(txt[4])
tokens(txt[4], remove_numbers = TRUE)
tokens(txt[4], remove_symbols = TRUE)
# note the difference
tokens(tokenize(txt[4]), remove_symbols = TRUE)

# using other tokenizers
tokenizers::tokenize_words(txt, lowercase = FALSE, strip_punct = FALSE) \%>\%
    tokens(remove_symbols = TRUE)
tokenizers::tokenize_characters(txt[3], strip_non_alphanum = FALSE) \%>\%
    tokens(remove_punct = TRUE)
tokenizers::tokenize_sentences(
    "The quick brown fox.  It jumped over the lazy dog.") \%>\%
    tokens()

# ngram tokenization
tokens(txt[1:2], remove_punct = TRUE, ngrams = 2)
tokens(txt[1:2], remove_punct = TRUE, ngrams = 2, skip = 1, concatenator = " ")
tokens(txt[1:2], remove_punct = TRUE, ngrams = 1:2)

# removing features from ngram tokens
tokens(txt, remove_punct = TRUE, ngrams = 1:2) \%>\%
    tokens_remove(stopwords("english"))
}
\seealso{
\code{\link[=tokens_ngrams]{tokens_ngrams()}}, \code{\link[=tokens_skipgrams]{tokens_skipgrams()}}, \code{\link[=as.list.tokens]{as.list.tokens()}}, \code{\link[=as.tokens]{as.tokens()}}
}
\keyword{tokens}
