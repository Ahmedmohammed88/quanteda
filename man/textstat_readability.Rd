% Generated by roxygen2: do not edit by hand
% Please edit documentation in R/textstat_readability.R
\name{textstat_readability}
\alias{textstat_readability}
\title{Calculate readability}
\usage{
textstat_readability(x, measure = c("all", "ARI", "ARI.simple",
  "Bormuth", "Bormuth.GP", "Coleman", "Coleman.C2", "Coleman.Liau",
  "Coleman.Liau.grade", "Coleman.Liau.short", "Dale.Chall",
  "Dale.Chall.old", "Dale.Chall.PSK", "Danielson.Bryan",
  "Danielson.Bryan.2", "Dickes.Steiwer", "DRP", "ELF",
  "Farr.Jenkins.Paterson", "Flesch", "Flesch.PSK", "Flesch.Kincaid", "FOG",
  "FOG.PSK", "FOG.NRI", "FORCAST", "FORCAST.RGL", "Fucks", "Linsear.Write",
  "LIW", "nWS", "nWS.2", "nWS.3", "nWS.4", "RIX", "Scrabble", "SMOG",
  "SMOG.C", "SMOG.simple",      "SMOG.de", "Spache", "Spache.old",
  "Strain", "Traenkle.Bailer", "Traenkle.Bailer.2", "Wheeler.Smith",
  "meanSentenceLength", "meanWordSyllables"), remove_hyphens = TRUE,
  min_sentence_length = 1, max_sentence_length = 10000,
  intermediate = FALSE, ...)
}
\arguments{
\item{x}{a character or \link{corpus} object containing the texts}

\item{measure}{character vector defining the readability measure to calculate.
Matches are case-insensitive.}

\item{remove_hyphens}{if \code{TRUE}, treat constituent words in hyphenated as
separate terms, for purposes of computing word lengths, e.g.
"decision-making" as two terms of lengths 8 and 6 characters respectively,
rather than as a single word of 15 characters}

\item{min_sentence_length, max_sentence_length}{set the minimum and maximum 
sentence lengths (in tokens, excluding punctuation) to include in the
computation of readability.  This makes it easy to exclude "sentences" that
may not really be sentences, such as section titles, table elements, and 
other cruft that might be in the texts following conversion.

For finer-grained control, consider filtering sentences prior first, 
including through pattern-matching, using \code{\link{corpus_trim}}.}

\item{intermediate}{if \code{TRUE}, include intermediate quantities in the output}

\item{...}{not used}
}
\value{
\code{textstat_readability} returns a data.frame of documents and
  their readability scores.
}
\description{
Calculate the readability of text(s) using one of a variety of computed 
indexes.
}
\details{
\code{textstat_readability} calculates the readability of documents 
  using a variety of indices

In the following formulas, we define  
  \deqn{Average Sentence Length = ASL = No. Of Words / No. of Sentences} 
  \deqn{Average Word Length = AWL = No. of Characters / No. of Words} 
  \deqn{Average Familiar Words = AFL = No. of Words in Dale-Chall List of 3000 Simple Words /  No. of Words} 
  \deqn{Difficult Words = Words not in the Dale-Chall List of 3000 Simple Words}
  
  \describe{
  \item{\code{"ARI"}:}{\emph{Automated Readability Index (1967)}: \deqn{ARI = 0.5 ASL  +
  4.71 AWL - 21.34}{ARI = 0.5 ASL  + 4.71 AWL - 21.34}}
  
  \item{\code{"ARI.Simple"}:}{\emph{Simplified Automated Readability Index}: \deqn{
  ARI.Simple =  ASL + 9 AWL}{ARI.Simple =  ASL + 9 AWL}}
  
  \item{\code{"Bormuth"}:}{\emph{Bormuth Mean Cloze Formula (1969)}: \deqn{Bormuth = 
  0.886593 - 0.03640 \times AWL + 0.161911 \times AFW  - 0.21401 \times ASL - 0.000577 \times ASL^2 - 
  0.000005 \times ASL^3 }{Bormuth = 0.886593 - 0.03640 x AWL + 0.161911 x AFW  - 0.21401 x 
  ASL - 0.000577 x ASL^2 -  0.000005 x ASL^3 }}
  
  \item{\code{"Bormuth.GP"}:}{\emph{Bormuth Grade Placement (1969)}: \deqn{Bormuth.GP = 
  4.275 + 12.881M - 34.934M^2 + 20.388 M^3 + 26.194CCS - 2.046CCS^2 - 11.767CCS^3 - 
  42.285(M \times CCS) + 97.620(M \times CCS)^2 - 59.538(M \times CCS)^2 where M is the Bormuth Mean Cloze
  Formula as in  \emph{"Bormuth"} above, and CCS is the Cloze Criterion Score (Bormuth, 1968)}{Bormuth.GP = 
  4.275 + 12.881M - 34.934M^2 + 20.388 M^3 + 26.194CCS - 2.046CCS^2 - 11.767CCS^3 - 
  42.285(M x CCS) + 97.620(M x CCS)^2 - 59.538(M x CCS)^2 where M is the Bormuth Mean Cloze
  Formula as in "Bormuth" above, and CCS is the Cloze Criterion Score (Bormuth, 1968)}}
 
  \item{\code{"Coleman"}:}{\emph{Coleman's Readability Formula 1 (1971)}: \deqn{Coleman = 
  1.29 \times \frac{100 \times No. of 1-Syllable Words}{No. of Words} - 38.45}{Coleman = 
  1.29 x (100 x No. of 1-Syllable Words / No. of Words) - 38.45}}
  
  \item{\code{"Coleman.C2"}:}{\emph{Coleman's Readability Formula 2 (1971)}: \deqn{Coleman2 = 
  1.16 \times \frac{100 \times No. of 1-Syllable Words}{No. of Words} + 1.48 \times
  \frac{100 \times No. of Sentences}{No. of Words} -37.95 }{Coleman2 = 
  1.16 x (100 x No. of 1-Syllable Words / No. of Words) + 1.48 x (100 x No. of Sentences / No.
   of Words) - 37.95}}

  \item{\code{"Coleman.Liau"}:}{\emph{Coleman-Liau Esimated Cloze Percent (ECP) (1975)}: \deqn{Coleman-Liau 
  ECP = 141.8401 - 0.214590 \times 100 \times AWL + 1.079812 \times \frac{No. of Sentences \times 100}{No. of
  Words}}{Coleman-Liau ECP =  141.8401 - (0.214590 x 100 x AWL) + (1.079812 x No. of Sentences x 100 / No. of
  Words)}}
  \item{\code{"Coleman.Liau.Grade"}:}{\emph{Coleman-Liau Grade Level (1975)}: \deqn{Coleman-Liau Grade = 
  -27.4004 \times Coleman.Liau.ECP \times 100 + 23.06395}{Coleman-Liau Grade =  -27.4004 x 
  Coleman.Liau.ECP / 100 + 23.06395}}
   
  \item{\code{"Coleman.Liau.Short"}:}{\emph{Coleman-Liau Index (1975)}: \deqn{Coleman-Liau.Short= 
  5.88 \times AWL + 29.6 \times \frac{No. of Sentences}{No. of Words} - 15.8}{Coleman-Liau.Short = 
  5.88 x Average Word Length + 0.296 x (No. of Sentences / No. of Words) - 15.8}}
   
  \item{\code{"Dale.Chall.Old"}:}{\emph{Dale-Chall (1948)}: \deqn{
  Dale.Chall.Old =  0.1579 \times 100 \times \frac{No. of Difficult Words}{No. of Words} + 
  0.0496 \times ASL (+ 3.6365)}{Dale.Chall.Old = 0.1579 x 100 x No. of Difficult Words / No.
   of Words  + 0.0496 x ASL (+3.6365 if No. of Difficult Words / No. of Words is > 0.05)}}
  
  \item{\code{"Danielson.Bryan"}:}{\emph{Danielson-Bryan (1963)}: \deqn{
  Danielson-Bryan = 1.0364 \times \frac{No. of Characters}{No. of Words} + 0.0194 \times
  \frac{No. of Characters}{No. of Sentences} - 0.6059}{Danielson-Bryan = 
  1.0364 x No. of Characters / No. of Words + 0.0194 x No. of Characters / No. of
   Sentences - 0.6059}}
  
  \item{\code{"Flesch"}:}{\emph{Flesch Reading Ease Score (FRES) (1948)}: \deqn{
  FRES = 206.835 - 1.015 \times ASL - 84.6 \times \frac(No. of Syllables / No. of Words)}{FRES =
  206.835 - 1.015 x ASL - 84.6 x (No. of Syllables / No. of Words)}}
  
  \item{\code{"Flesch.Kincaid"}:}{\emph{Flesch-Kincaid Score (1975)}: \deqn{
  Flesch-Kincaid = 0.39 \times ASL + 11.8  \times \frac{No. of Syllables}{No. of Words}- 15.59}{Flesch-Kincaid =
  0.39 x ASL + 11.8  x(No. of Syllables / No. of Words) - 15.59}}
  
  \item{\code{"Fucks"}:}{\emph{Fucks' Stilcharakteristik (Style Characteristic)}: \deqn{Fucks = 
  AWL  \times ASL }{Fucks = AWL x ASL}}
  
  \item{\code{"FOG"}:}{\emph{Gunning's Fog Index (1952)}: \deqn{FOG = 
  0.4 \times (ASL + 100 \times (Complex Words / No. of Words))}{FOG = 
  0.4 x (ASL + 100 x (Complex Words / No. of Words))}}
  
  \item{\code{"FORCAST"}:}{\emph{FORCAST (1973)}: \deqn{FORCAST = 20 - 
  \frac{(No. of Single Syllables Words \times 150)}{(No. of Words \times 10)}}{FORCAST = 20 - 
  (No. of Single Syllables Words x 150) / (No. of Words x 10)(The scaling by 150 arises because the 
  initial FORCAST index is based on just a sample of 150 tokens)}}
  
  \item{\code{"Scrabble"}:}{\emph{Scrabble Measure}: {Scrabble = Mean Scrabble Letter Values of All Words}}
  
  \item{\code{"SMOG"}:}{\emph{Simple Measure of Gobbledygook (1969)}: \deqn{SMOG = 1.043 \times \sqrt{No. of
   Words with >3 Syllables \times \frac{30}{No. of Sentences}} + 3.1291}{SMOG = 1.043
   x sqrt(No. of Words with >3 Syllables x 30 / No. Of Sentences) + 3.1291}}
   
  \item{\code{"SMOG.Simple"}:}{\emph{Simplified Version of SMOG (1969)}: \deqn{SMOG.Simple =
  \sqrt{No. of Words with >3 Syllables \times \frac{30}{No. of Sentences}} + 3}{SMOG.Simple = 
  sqrt(No. of Words with >3 Syllables x 30 / No. of Sentences) + 3}}
  
  \item{\code{"Spache"}:}{\emph{Spache (1952)}: \deqn{Spache = 0.121 \times ASL + 
  0.082 \times (Unique Words not in Spache Word List / No. of Words) + 0.659}{Spache = 0.121 x ASL + 
  0.082 x (Unique Words not in Spache Word List / No. of Words) + 0.659}} 
  
  \item{\code{"Spache.old"}:}{\emph{Spache (1952)}: \deqn{Spache.old = 0.141 \times ASL + 
  0.086 \times Unique Words not in Spache Word List / No. of Words) + 0.839}{Spache.old = 0.141 x ASL + 
  0.086 x (Unique Words not in Spache Word List/ No. of Words) + 0.839}} 
  
  \item{\code{"Strain"}:}{\emph{Strain Index (2006)}: \deqn{ Strain = Number of 
  Syllables / (Number of Sentences / 3) /10}{Strain = Number of Syllables / (Number of 
  Sentences / 3) / 10 (The scaling by 3 arises because the initial Strain index is based on just the 
  first 3 sentences )}}
  
  \item{\code{"Traenkle.Bailer"}:}{\emph{Tränkle & Bailer (1984)}: \deqn{Tränkle.Bailer = 
  224.6814 - (79.8304 \times AWL) - (12.24032 \times ASL) - (1.292857 \times 100 
  \times \frac{No. of Prepositions}{No. of Words}}{Tränkle.Bailer = 224.6814 - (79.8304 x AWL) + 
  (12.24032 x ASL) - (1.292857 x 100 x No. of Prepositions / No. of Words)}}
  
  \item{\code{"Traenkle.Bailer2"}:}{\emph{Tränkle & Bailer (1984)}: \deqn{Tränkle.Bailer2 = 
  Tränkle.Bailer2 =  234.1063 - (96.11069 \times AWL ) - (2.05444 \times 100 \times \frac{No. of
   Prepositions}{No. of Words}) - (1.02805 \times 100 \times \frac{No. of Conjucntions}{No. of 
   Words}}{Tränkle.Bailer2 = 234.1063 - 96.11069 x AWL  - 2.05444 x 100 x (No. of Prepositions / No.
    of Words) - 1.02805 x 100 x (No. of Conjunctions/No. of Words)}}
  
  \item{\code{"Wheeler.Smith"}:}{\emph{Wheeler & Smith (1954)}: \deqn{Wheeler.Smith = 
  ASL \times 10 \times \frac{No. of 2-Syllables Words}{No. of Words}}{Wheeler.Smith = 
  ASL x 10 x (No. of 2-Syllables Words /No. of Words)}}
  
  \item{\code{"meanSentenceLength"}:}{\emph{Average Sentence Length}: \deqn{Average Sentence Length = 
  \frac{No. of Words}{No. of Sentences}}{Average Sentence Length = No. of Words / No. of Sentences}}
  
  \item{\code{"meanWordSyllables"}:}{\emph{Average Word Syllables}: \deqn{Average Word Syllables = 
  \frac{No. of Syllables}{No. of Words}}{Average Word Syllables = No. of Syllables / No. of Words}}
  
  }
}
\examples{
txt <- c(doc1 = "Readability zero one. Ten, Eleven.", 
         doc2 = "The cat in a dilapidated tophat.")
textstat_readability(txt, measure = "Flesch")
textstat_readability(txt, measure = c("FOG", "FOG.PSK", "FOG.NRI"))

textstat_readability(data_corpus_inaugural[48:58], 
                     measure = c("Flesch.Kincaid", "Dale.Chall.old"))
}
\references{
Bormuth, J. R. (1969). \href{https://files.eric.ed.gov/fulltext/ED029166.pdf}{Development 
  of Readability Analysis.}  
  
  Bormuth, J. R. (1968). Cloze test readability: Criterion reference scores. 
  Journal of educational measurement, 5(3), 189-196.  
  
  Caylor, J. S. (1973). Methodologies for Determining Reading Requirements of Military 
  Occupational Specialties.
  
  Coleman, E. B. (1971). Developing a technology of written instruction: Some determiners 
  of the complexity of prose. Verbal learning research and the technology of written 
  instruction, 155-204.
  
  Coleman, M., & Liau, T. L. (1975). A computer readability formula designed for machine
  scoring. Journal of Applied Psychology, 60(2), 283.
  
  Dale, E., & Chall, J. S. (1948). A formula for predicting readability: Instructions.
  Educational research bulletin, 37-54.
  
  Danielson, W. A., & Bryan, S. D. (1963). Computer automation of two readability formulas.
  Journalism Quarterly, 40(2), 201-206.
  
  DuBay, W. H. (2004). The Principles of Readability.
  
  Flesch, R. (1948). A new readability yardstick. Journal of applied psychology, 32(3), 221.
  
  Gunning, R. (1952). The technique of clear writing.
  
  Kincaid, J. P., Fishburne Jr, R. P., Rogers, R. L., & Chissom, B. S. (1975). Derivation of 
  new readability formulas (automated readability index, fog count and flesch reading ease formula) 
  for navy enlisted personnel.
  
  Mc Laughlin, G. H. (1969). SMOG grading-a new readability formula. Journal of reading, 12(8), 639-646.
  
  Senter, R. J., & Smith, E. A. (1967). Automated readability index. CINCINNATI UNIV OH.
  
  Solomon, N. W. (2006). Qualitative Analysis of Media Language. India.
}
\author{
Kenneth Benoit, re-engineered from Meik Michalke's \pkg{koRpus}
  package.
}
