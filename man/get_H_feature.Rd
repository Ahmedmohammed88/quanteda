% Generated by roxygen2: do not edit by hand
% Please edit documentation in R/textmodel_nb.R
\name{get_H_feature}
\alias{get_H_feature}
\title{Compute H for a word and its complement}
\usage{
get_H_feature(feature, model, base)
}
\arguments{
\item{feature}{character; the name of the feature for which H will be computed}

\item{model}{a \link{textmodel_nb} object}

\item{base}{base for logarithm function}
}
\value{
a vector of length 2, named "<feature>" and "~<feature>",
  corresponding to the two Pr(w) weighted terms in the formula for the
  computation of G(w).  See Jurafsky and Martin (2018, 17) and 
  Yang and Pedersen (1997).
}
\description{
Computes the conditional entropy quantities H for a word and for its
complement, from a \link{textmodel_nb} object.  Used for the multinomial
variant of Naive Bayes, to compute the information gain for a single feature.
}
\references{
Jurafsky, Daniel and James H. Martin.  (2018).
\href{https://web.stanford.edu/~jurafsky/slp3/4.pdf}{\emph{Speech and
Language Processing}}.   Stanford University typescript.  Draft of September
23.

Yang, Y. and Pedersen, J. O. (1997).
\href{http://www.surdeanu.info/mihai/teaching/ista555-spring15/readings/yang97comparative.pdf}{A
comparative study on feature selection in text categorization.} In
\emph{ICML} 97(412-420, July).
}
\keyword{internal}
\keyword{textmodel}
